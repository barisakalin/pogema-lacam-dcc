{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTyLFHcX9GCF",
        "outputId": "bf61a4fe-fbab-4d81-b801-3b4d88c05bf8",
        "collapsed": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m104.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m84.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m89.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EpwXQlfz9Z1Z"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision.models as models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivhrfq0uGQRr"
      },
      "source": [
        "#config.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "QUsfcEmD_gy6"
      },
      "outputs": [],
      "source": [
        "############################################################\n",
        "####################    environment     ####################\n",
        "############################################################\n",
        "\n",
        "obs_radius = 4\n",
        "reward_fn = dict(move=-0.075,\n",
        "                stay_on_goal=0,\n",
        "                stay_off_goal=-0.075,\n",
        "                collision=-0.5,\n",
        "                finish=3)\n",
        "\n",
        "obs_shape = (6, 2*obs_radius+1, 2*obs_radius+1)\n",
        "action_dim = 5\n",
        "\n",
        "############################################################\n",
        "####################         DQN        ####################\n",
        "############################################################\n",
        "\n",
        "# basic training setting\n",
        "num_actors = 16\n",
        "log_interval = 10\n",
        "training_steps = 150000\n",
        "save_interval = 1000\n",
        "gamma = 0.99\n",
        "batch_size = 128\n",
        "learning_starts = 50000\n",
        "target_network_update_freq = 1750\n",
        "save_path='./saved_models'\n",
        "max_episode_length = 256\n",
        "buffer_capacity = 262144\n",
        "chunk_capacity = 64\n",
        "burn_in_steps = 20\n",
        "\n",
        "actor_update_steps = 200\n",
        "\n",
        "# gradient norm clipping\n",
        "grad_norm_dqn=40\n",
        "\n",
        "# n-step forward\n",
        "forward_steps = 2\n",
        "\n",
        "# prioritized replay\n",
        "prioritized_replay_alpha=0.6\n",
        "prioritized_replay_beta=0.4\n",
        "\n",
        "# curriculum learning\n",
        "init_env_settings = (1, 10)\n",
        "max_num_agents = 16\n",
        "max_map_lenght = 40\n",
        "pass_rate = 0.9\n",
        "\n",
        "# dqn network setting\n",
        "cnn_channel = 128\n",
        "hidden_dim = 256\n",
        "\n",
        "# same as DHC if set to false\n",
        "selective_comm = True\n",
        "# only works if selective_comm set to false\n",
        "max_comm_agents = 3\n",
        "\n",
        "# curriculum learning\n",
        "cl_history_size = 100\n",
        "\n",
        "test_seed = 0\n",
        "num_test_cases = 200\n",
        "test_env_settings = (\n",
        "                    (40, 4, 0.3), (40, 8, 0.3),\n",
        "                    (40, 16, 0.3),\n",
        "                    (40, 32, 0.3),\n",
        "                    (40, 64, 0.3),\n",
        "                    (80, 4, 0.3), (80, 8, 0.3), (80, 16, 0.3), (80, 32, 0.3), (80, 64, 0.3),\n",
        "                    ) # map length, number of agents, density"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNZQNdjuGT3U"
      },
      "source": [
        "#environment.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "MvgGMVDsGWsr"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from typing import List, Union\n",
        "import numpy as np\n",
        "#import config\n",
        "\n",
        "ACTION_LIST = np.array([[-1, 0], [1, 0], [0, -1], [0, 1], [0, 0]], dtype=np.int32)\n",
        "\n",
        "class Environment:\n",
        "    def __init__(self, num_agents: int = init_env_settings[0], map_length: int = init_env_settings[1],\n",
        "                obs_radius: int = obs_radius, reward_fn: dict = reward_fn, fix_density=None,\n",
        "                curriculum=False, init_env_settings_set=init_env_settings):\n",
        "\n",
        "        self.curriculum = curriculum\n",
        "        if curriculum:\n",
        "            self.env_set = [init_env_settings_set]\n",
        "            self.num_agents = init_env_settings_set[0]\n",
        "            self.map_size = (init_env_settings_set[1], init_env_settings_set[1])\n",
        "        else:\n",
        "            self.num_agents = num_agents\n",
        "            self.map_size = (map_length, map_length)\n",
        "\n",
        "        # set as same as in PRIMAL\n",
        "        if fix_density is None:\n",
        "            self.fix_density = False\n",
        "            self.obstacle_density = np.random.triangular(0, 0.33, 0.5)\n",
        "        else:\n",
        "            self.fix_density = True\n",
        "            self.obstacle_density = fix_density\n",
        "\n",
        "        self.map = np.random.choice(2, self.map_size, p=[1-self.obstacle_density, self.obstacle_density]).astype(int)\n",
        "\n",
        "        partition_list = self._map_partition()\n",
        "\n",
        "        while len(partition_list) == 0:\n",
        "            self.map = np.random.choice(2, self.map_size, p=[1-self.obstacle_density, self.obstacle_density]).astype(int)\n",
        "            partition_list = self._map_partition()\n",
        "\n",
        "        self.agents_pos = np.empty((self.num_agents, 2), dtype=np.int32)\n",
        "        self.goals_pos = np.empty((self.num_agents, 2), dtype=np.int32)\n",
        "\n",
        "        pos_num = sum([len(partition) for partition in partition_list])\n",
        "\n",
        "        # loop to assign agent original position and goal position for each agent\n",
        "        for i in range(self.num_agents):\n",
        "\n",
        "            pos_idx = random.randint(0, pos_num-1)\n",
        "            partition_idx = 0\n",
        "            for partition in partition_list:\n",
        "                if pos_idx >= len(partition):\n",
        "                    pos_idx -= len(partition)\n",
        "                    partition_idx += 1\n",
        "                else:\n",
        "                    break\n",
        "\n",
        "            pos = random.choice(partition_list[partition_idx])\n",
        "            partition_list[partition_idx].remove(pos)\n",
        "            self.agents_pos[i] = np.asarray(pos, dtype=np.int32)\n",
        "\n",
        "            pos = random.choice(partition_list[partition_idx])\n",
        "            partition_list[partition_idx].remove(pos)\n",
        "            self.goals_pos[i] = np.asarray(pos, dtype=np.int32)\n",
        "\n",
        "            partition_list = [partition for partition in partition_list if len(partition) >= 2]\n",
        "            pos_num = sum([len(partition) for partition in partition_list])\n",
        "\n",
        "        self.obs_radius = obs_radius\n",
        "\n",
        "        self.reward_fn = reward_fn\n",
        "        self._get_heuri_map()\n",
        "        self.steps = 0\n",
        "\n",
        "        self.last_actions = np.zeros((self.num_agents, 5), dtype=bool)\n",
        "\n",
        "\n",
        "    def update_env_settings_set(self, new_env_settings_set):\n",
        "        self.env_set = new_env_settings_set\n",
        "\n",
        "    def reset(self, num_agents=None, map_length=None):\n",
        "\n",
        "        if self.curriculum:\n",
        "            rand = random.choice(self.env_set)\n",
        "            self.num_agents = rand[0]\n",
        "            self.map_size = (rand[1], rand[1])\n",
        "\n",
        "        elif num_agents is not None and map_length is not None:\n",
        "            self.num_agents = num_agents\n",
        "            self.map_size = (map_length, map_length)\n",
        "\n",
        "        if not self.fix_density:\n",
        "            self.obstacle_density = np.random.triangular(0, 0.33, 0.5)\n",
        "\n",
        "        self.map = np.random.choice(2, self.map_size, p=[1-self.obstacle_density, self.obstacle_density]).astype(np.float32)\n",
        "\n",
        "        partition_list = self._map_partition()\n",
        "\n",
        "        while len(partition_list) == 0:\n",
        "            self.map = np.random.choice(2, self.map_size, p=[1-self.obstacle_density, self.obstacle_density]).astype(np.float32)\n",
        "            partition_list = self._map_partition()\n",
        "\n",
        "        self.agents_pos = np.empty((self.num_agents, 2), dtype=np.int)\n",
        "        self.goals_pos = np.empty((self.num_agents, 2), dtype=np.int)\n",
        "\n",
        "        pos_num = sum([len(partition) for partition in partition_list])\n",
        "\n",
        "        for i in range(self.num_agents):\n",
        "\n",
        "            pos_idx = random.randint(0, pos_num-1)\n",
        "            partition_idx = 0\n",
        "            for partition in partition_list:\n",
        "                if pos_idx >= len(partition):\n",
        "                    pos_idx -= len(partition)\n",
        "                    partition_idx += 1\n",
        "                else:\n",
        "                    break\n",
        "\n",
        "            pos = random.choice(partition_list[partition_idx])\n",
        "            partition_list[partition_idx].remove(pos)\n",
        "            self.agents_pos[i] = np.asarray(pos, dtype=np.int)\n",
        "\n",
        "            pos = random.choice(partition_list[partition_idx])\n",
        "            partition_list[partition_idx].remove(pos)\n",
        "            self.goals_pos[i] = np.asarray(pos, dtype=np.int)\n",
        "\n",
        "            partition_list = [partition for partition in partition_list if len(partition) >= 2]\n",
        "            pos_num = sum([len(partition) for partition in partition_list])\n",
        "\n",
        "        self.steps = 0\n",
        "        self._get_heuri_map()\n",
        "\n",
        "        self.last_actions = np.zeros((self.num_agents, 5), dtype=np.bool)\n",
        "\n",
        "        return self.observe()\n",
        "\n",
        "    def load(self, map:np.ndarray, agents_pos:np.ndarray, goals_pos:np.ndarray):\n",
        "\n",
        "        self.map = np.copy(map)\n",
        "        self.agents_pos = np.copy(agents_pos)\n",
        "        self.goals_pos = np.copy(goals_pos)\n",
        "\n",
        "        self.num_agents = agents_pos.shape[0]\n",
        "        self.map_size = (self.map.shape[0], self.map.shape[1])\n",
        "\n",
        "        self.steps = 0\n",
        "\n",
        "        self._get_heuri_map()\n",
        "\n",
        "        self.last_actions = np.zeros((self.num_agents, 5), dtype=bool)\n",
        "\n",
        "    def _get_heuri_map(self):\n",
        "        dist_map = np.ones((self.num_agents, *self.map_size), dtype=np.int32) * np.iinfo(np.int32).max\n",
        "\n",
        "        empty_pos = np.argwhere(self.map==0).tolist()\n",
        "        empty_pos = set([tuple(pos) for pos in empty_pos])\n",
        "\n",
        "        for i in range(self.num_agents):\n",
        "            open_list = set()\n",
        "            x, y = tuple(self.goals_pos[i])\n",
        "            open_list.add((x, y))\n",
        "            dist_map[i, x, y] = 0\n",
        "\n",
        "            while open_list:\n",
        "                x, y = open_list.pop()\n",
        "                dist = dist_map[i, x, y]\n",
        "\n",
        "                up = x-1, y\n",
        "                if up in empty_pos and dist_map[i, x-1, y] > dist+1:\n",
        "                    dist_map[i, x-1, y] = dist+1\n",
        "                    open_list.add(up)\n",
        "\n",
        "                down = x+1, y\n",
        "                if down in empty_pos and dist_map[i, x+1, y] > dist+1:\n",
        "                    dist_map[i, x+1, y] = dist+1\n",
        "                    open_list.add(down)\n",
        "\n",
        "                left = x, y-1\n",
        "                if left in empty_pos and dist_map[i, x, y-1] > dist+1:\n",
        "                    dist_map[i, x, y-1] = dist+1\n",
        "                    open_list.add(left)\n",
        "\n",
        "                right = x, y+1\n",
        "                if right in empty_pos and dist_map[i, x, y+1] > dist+1:\n",
        "                    dist_map[i, x, y+1] = dist+1\n",
        "                    open_list.add(right)\n",
        "\n",
        "        self.heuri_map = np.zeros((self.num_agents, 4, *self.map_size), dtype=bool)\n",
        "        #AAAself.heuri_map = np.zeros((self.num_agents, 4, *self.map_size), dtype=np.bool)\n",
        "\n",
        "        for x, y in empty_pos:\n",
        "            for i in range(self.num_agents):\n",
        "\n",
        "                if x > 0 and dist_map[i, x-1, y] < dist_map[i, x, y]:\n",
        "                    self.heuri_map[i, 0, x, y] = 1\n",
        "\n",
        "                if x < self.map_size[0]-1 and dist_map[i, x+1, y] < dist_map[i, x, y]:\n",
        "                    self.heuri_map[i, 1, x, y] = 1\n",
        "\n",
        "                if y > 0 and dist_map[i, x, y-1] < dist_map[i, x, y]:\n",
        "                    self.heuri_map[i, 2, x, y] = 1\n",
        "\n",
        "                if y < self.map_size[1]-1 and dist_map[i, x, y+1] < dist_map[i, x, y]:\n",
        "                    self.heuri_map[i, 3, x, y] = 1\n",
        "\n",
        "        self.heuri_map = np.pad(self.heuri_map, ((0, 0), (0, 0), (self.obs_radius, self.obs_radius), (self.obs_radius, self.obs_radius)))\n",
        "\n",
        "    def _map_partition(self):\n",
        "        '''\n",
        "        partitioning map into independent partitions\n",
        "        '''\n",
        "        empty_list = np.argwhere(self.map==0).tolist()\n",
        "\n",
        "        empty_pos = set([tuple(pos) for pos in empty_list])\n",
        "\n",
        "        if not empty_pos:\n",
        "            raise RuntimeError('no empty position')\n",
        "\n",
        "        partition_list = list()\n",
        "        while empty_pos:\n",
        "\n",
        "            start_pos = empty_pos.pop()\n",
        "\n",
        "            open_list = list()\n",
        "            open_list.append(start_pos)\n",
        "            close_list = list()\n",
        "\n",
        "            while open_list:\n",
        "                x, y = open_list.pop(0)\n",
        "\n",
        "                up = x-1, y\n",
        "                if up in empty_pos:\n",
        "                    empty_pos.remove(up)\n",
        "                    open_list.append(up)\n",
        "\n",
        "                down = x+1, y\n",
        "                if down in empty_pos:\n",
        "                    empty_pos.remove(down)\n",
        "                    open_list.append(down)\n",
        "\n",
        "                left = x, y-1\n",
        "                if left in empty_pos:\n",
        "                    empty_pos.remove(left)\n",
        "                    open_list.append(left)\n",
        "\n",
        "                right = x, y+1\n",
        "                if right in empty_pos:\n",
        "                    empty_pos.remove(right)\n",
        "                    open_list.append(right)\n",
        "\n",
        "                close_list.append((x, y))\n",
        "\n",
        "            if len(close_list) >= 2:\n",
        "                partition_list.append(close_list)\n",
        "\n",
        "        return partition_list\n",
        "\n",
        "    def step(self, actions: List[int]):\n",
        "        '''\n",
        "        actions:\n",
        "            list of indices\n",
        "                0 up\n",
        "                1 down\n",
        "                2 left\n",
        "                3 right\n",
        "                4 stay\n",
        "        '''\n",
        "\n",
        "        assert len(actions) == self.num_agents, 'only {} actions as input while {} agents in environment'.format(len(actions), self.num_agents)\n",
        "        assert all([action_idx<5 and action_idx>=0 for action_idx in actions]), 'action index out of range'\n",
        "\n",
        "        checking_list = [i for i in range(self.num_agents)]\n",
        "\n",
        "        rewards = []\n",
        "        next_pos = np.copy(self.agents_pos)\n",
        "\n",
        "        # remove unmoving agent id\n",
        "        for agent_id in checking_list.copy():\n",
        "            if actions[agent_id] == 4:\n",
        "                # unmoving\n",
        "                if np.array_equal(self.agents_pos[agent_id], self.goals_pos[agent_id]):\n",
        "                    rewards.append(self.reward_fn['stay_on_goal'])\n",
        "                else:\n",
        "                    rewards.append(self.reward_fn['stay_off_goal'])\n",
        "                checking_list.remove(agent_id)\n",
        "            else:\n",
        "                # move\n",
        "                next_pos[agent_id] += ACTION_LIST[actions[agent_id]]\n",
        "                rewards.append(self.reward_fn['move'])\n",
        "\n",
        "        # first round check, these two conflicts have the highest priority\n",
        "        for agent_id in checking_list.copy():\n",
        "\n",
        "            if np.any(next_pos[agent_id]<0) or np.any(next_pos[agent_id]>=self.map_size[0]):\n",
        "                # agent out of map range\n",
        "                rewards[agent_id] = self.reward_fn['collision']\n",
        "                next_pos[agent_id] = self.agents_pos[agent_id]\n",
        "                checking_list.remove(agent_id)\n",
        "\n",
        "            elif self.map[tuple(next_pos[agent_id])] == 1:\n",
        "                # collide obstacle\n",
        "                rewards[agent_id] = self.reward_fn['collision']\n",
        "                next_pos[agent_id] = self.agents_pos[agent_id]\n",
        "                checking_list.remove(agent_id)\n",
        "\n",
        "        # second round check, agent swapping conflict\n",
        "        no_conflict = False\n",
        "        while not no_conflict:\n",
        "\n",
        "            no_conflict = True\n",
        "            for agent_id in checking_list:\n",
        "\n",
        "                target_agent_id = np.where(np.all(next_pos[agent_id]==self.agents_pos, axis=1))[0]\n",
        "\n",
        "                #if target_agent_id:\n",
        "                if target_agent_id is not None and len(target_agent_id) > 0:\n",
        "\n",
        "                    target_agent_id = target_agent_id.item()\n",
        "\n",
        "                    if np.array_equal(next_pos[target_agent_id], self.agents_pos[agent_id]):\n",
        "                        assert target_agent_id in checking_list, 'target_agent_id should be in checking list'\n",
        "\n",
        "                        next_pos[agent_id] = self.agents_pos[agent_id]\n",
        "                        rewards[agent_id] = self.reward_fn['collision']\n",
        "\n",
        "                        next_pos[target_agent_id] = self.agents_pos[target_agent_id]\n",
        "                        rewards[target_agent_id] = self.reward_fn['collision']\n",
        "\n",
        "                        checking_list.remove(agent_id)\n",
        "                        checking_list.remove(target_agent_id)\n",
        "\n",
        "                        no_conflict = False\n",
        "                        break\n",
        "\n",
        "        # third round check, agent collision conflict\n",
        "        no_conflict = False\n",
        "        while not no_conflict:\n",
        "            no_conflict = True\n",
        "            for agent_id in checking_list:\n",
        "\n",
        "                collide_agent_id = np.where(np.all(next_pos==next_pos[agent_id], axis=1))[0].tolist()\n",
        "                if len(collide_agent_id) > 1:\n",
        "                    # collide agent\n",
        "\n",
        "                    # if all agents in collide agent are in checking list\n",
        "                    all_in_checking = True\n",
        "                    for id in collide_agent_id.copy():\n",
        "                        if id not in checking_list:\n",
        "                            all_in_checking = False\n",
        "                            collide_agent_id.remove(id)\n",
        "\n",
        "                    if all_in_checking:\n",
        "\n",
        "                        collide_agent_pos = next_pos[collide_agent_id].tolist()\n",
        "                        for pos, id in zip(collide_agent_pos, collide_agent_id):\n",
        "                            pos.append(id)\n",
        "                        collide_agent_pos.sort(key=lambda x: x[0]*self.map_size[0]+x[1])\n",
        "\n",
        "                        collide_agent_id.remove(collide_agent_pos[0][2])\n",
        "\n",
        "                    next_pos[collide_agent_id] = self.agents_pos[collide_agent_id]\n",
        "                    for id in collide_agent_id:\n",
        "                        rewards[id] = self.reward_fn['collision']\n",
        "\n",
        "                    for id in collide_agent_id:\n",
        "                        checking_list.remove(id)\n",
        "\n",
        "                    no_conflict = False\n",
        "                    break\n",
        "\n",
        "        self.agents_pos = np.copy(next_pos)\n",
        "\n",
        "        self.steps += 1\n",
        "\n",
        "        # check done\n",
        "        if np.array_equal(self.agents_pos, self.goals_pos):\n",
        "            done = True\n",
        "            rewards = [self.reward_fn['finish'] for _ in range(self.num_agents)]\n",
        "        else:\n",
        "            done = False\n",
        "\n",
        "        info = {'step': self.steps-1}\n",
        "\n",
        "        # make sure no overlapping agents\n",
        "        assert np.unique(self.agents_pos, axis=0).shape[0] == self.num_agents\n",
        "\n",
        "        # update last actions\n",
        "        self.last_actions = np.zeros((self.num_agents, 5), dtype=bool)\n",
        "        #AAAself.last_actions = np.zeros((self.num_agents, 5), dtype=np.bool)\n",
        "        self.last_actions[np.arange(self.num_agents), np.array(actions)] = 1\n",
        "\n",
        "        return self.observe(), rewards, done, info\n",
        "\n",
        "\n",
        "    def observe(self):\n",
        "        '''\n",
        "        return observation and position for each agent\n",
        "\n",
        "        obs: shape (num_agents, 6, 2*obs_radius+1, 2*obs_radius+1)\n",
        "            layer 1: agent map\n",
        "            layer 2: obstacle map\n",
        "            layer 3-6: heuristic map\n",
        "\n",
        "        last_act: agents' last step action\n",
        "\n",
        "        pos: current position of each agent, used for caculating communication mask\n",
        "\n",
        "        '''\n",
        "        obs = np.zeros((self.num_agents, 6, 2*self.obs_radius+1, 2*self.obs_radius+1), dtype=bool)\n",
        "\n",
        "        obstacle_map = np.pad(self.map, self.obs_radius, 'constant', constant_values=0)\n",
        "\n",
        "        agent_map = np.zeros((self.map_size), dtype=bool)\n",
        "        agent_map[self.agents_pos[:,0], self.agents_pos[:,1]] = 1\n",
        "        agent_map = np.pad(agent_map, self.obs_radius, 'constant', constant_values=0)\n",
        "\n",
        "        for i, agent_pos in enumerate(self.agents_pos):\n",
        "            x, y = agent_pos\n",
        "\n",
        "            obs[i, 0] = agent_map[x:x+2*self.obs_radius+1, y:y+2*self.obs_radius+1]\n",
        "            obs[i, 0, self.obs_radius, self.obs_radius] = 0\n",
        "            obs[i, 1] = obstacle_map[x:x+2*self.obs_radius+1, y:y+2*self.obs_radius+1]\n",
        "            obs[i, 2:] = self.heuri_map[i, :, x:x+2*self.obs_radius+1, y:y+2*self.obs_radius+1]\n",
        "\n",
        "        return obs, np.copy(self.last_actions), np.copy(self.agents_pos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmA1awDo9lYp"
      },
      "source": [
        "#buffer.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "TmrZRJuM9m4m"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from dataclasses import dataclass\n",
        "import numpy as np\n",
        "\n",
        "@dataclass\n",
        "class EpisodeData:\n",
        "    __slots__ = ('actor_id', 'num_agents', 'map_len', 'obs', 'last_act', 'actions', 'rewards',\n",
        "                'hiddens', 'relative_pos', 'comm_mask', 'gammas', 'td_errors', 'sizes', 'done')\n",
        "    actor_id: int\n",
        "    num_agents: int\n",
        "    map_len: int\n",
        "    obs: np.ndarray\n",
        "    last_act: np.ndarray\n",
        "    actions: np.ndarray\n",
        "    rewards: np.ndarray\n",
        "    hiddens: np.ndarray\n",
        "    relative_pos: np.ndarray\n",
        "    comm_mask: np.ndarray\n",
        "    gammas: np.ndarray\n",
        "    td_errors: np.ndarray\n",
        "    sizes: np.ndarray\n",
        "    done: bool\n",
        "\n",
        "\n",
        "class SumTree:\n",
        "    '''used for prioritized experience replay'''\n",
        "    def __init__(self, capacity: int):\n",
        "        layer = 1\n",
        "        while 2**(layer-1) < capacity:\n",
        "            layer += 1\n",
        "        assert 2**(layer-1) == capacity, 'capacity only allow n**2 size'\n",
        "        self.layer = layer\n",
        "        self.tree = np.zeros(2**layer-1, dtype=np.float64)\n",
        "        self.capacity = capacity\n",
        "        self.size = 0\n",
        "\n",
        "    def sum(self):\n",
        "        assert np.sum(self.tree[-self.capacity:])-self.tree[0] < 0.1, 'sum is {} but root is {}'.format(np.sum(self.tree[-self.capacity:]), self.tree[0])\n",
        "        return self.tree[0]\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        assert 0 <= idx < self.capacity\n",
        "\n",
        "        return self.tree[self.capacity-1+idx]\n",
        "\n",
        "    def batch_sample(self, batch_size: int):\n",
        "        p_sum = self.tree[0]\n",
        "        interval = p_sum/batch_size\n",
        "\n",
        "        prefixsums = np.arange(0, p_sum, interval, dtype=np.float64) + np.random.uniform(0, interval, batch_size)\n",
        "\n",
        "        idxes = np.zeros(batch_size, dtype=np.int)\n",
        "        for _ in range(self.layer-1):\n",
        "            nodes = self.tree[idxes*2+1]\n",
        "            idxes = np.where(prefixsums<nodes, idxes*2+1, idxes*2+2)\n",
        "            prefixsums = np.where(idxes%2==0, prefixsums-self.tree[idxes-1], prefixsums)\n",
        "\n",
        "        priorities = self.tree[idxes]\n",
        "        idxes -= self.capacity-1\n",
        "\n",
        "        assert np.all(priorities>0), 'idx: {}, priority: {}'.format(idxes, priorities)\n",
        "        assert np.all(idxes>=0) and np.all(idxes<self.capacity)\n",
        "\n",
        "        return idxes, priorities\n",
        "\n",
        "    def batch_update(self, idxes: np.ndarray, priorities: np.ndarray):\n",
        "        assert idxes.shape[0] == priorities.shape[0]\n",
        "        idxes += self.capacity-1\n",
        "        self.tree[idxes] = priorities\n",
        "\n",
        "        for _ in range(self.layer-1):\n",
        "            idxes = (idxes-1) // 2\n",
        "            idxes = np.unique(idxes)\n",
        "            self.tree[idxes] = self.tree[2*idxes+1] + self.tree[2*idxes+2]\n",
        "\n",
        "        # check\n",
        "        assert np.sum(self.tree[-self.capacity:])-self.tree[0] < 0.1, 'sum is {} but root is {}'.format(np.sum(self.tree[-self.capacity:]), self.tree[0])\n",
        "\n",
        "\n",
        "class LocalBuffer:\n",
        "    __slots__ = ('actor_id', 'map_len', 'num_agents', 'obs_buf', 'act_buf', 'rew_buf', 'hidden_buf', 'forward_steps',\n",
        "                'relative_pos_buf', 'q_buf', 'capacity', 'size', 'done', 'burn_in_steps', 'chunk_capacity', 'last_act_buf', 'comm_mask_buf')\n",
        "    def __init__(self, actor_id: int, num_agents: int, map_len: int, init_obs: np.ndarray, forward_steps=forward_steps,\n",
        "                capacity: int = max_episode_length, burn_in_steps=burn_in_steps,\n",
        "                obs_shape=obs_shape, hidden_dim=hidden_dim, action_dim=action_dim):\n",
        "        \"\"\"\n",
        "        buffer for each episode\n",
        "        \"\"\"\n",
        "        self.actor_id = actor_id\n",
        "        self.num_agents = num_agents\n",
        "        self.map_len = map_len\n",
        "\n",
        "        self.burn_in_steps = burn_in_steps\n",
        "        self.forward_steps = forward_steps\n",
        "\n",
        "        self.chunk_capacity = chunk_capacity\n",
        "\n",
        "        self.obs_buf = np.zeros((burn_in_steps+capacity+1, num_agents, *obs_shape), dtype=np.bool)\n",
        "        self.last_act_buf = np.zeros((burn_in_steps+capacity+1, num_agents, 5), dtype=np.bool)\n",
        "        self.act_buf = np.zeros((capacity), dtype=np.uint8)\n",
        "        self.rew_buf = np.zeros((capacity+forward_steps-1), dtype=np.float16)\n",
        "        self.hidden_buf = np.zeros((burn_in_steps+capacity+1, num_agents, hidden_dim), dtype=np.float16)\n",
        "        self.relative_pos_buf = np.zeros((burn_in_steps+capacity+1, num_agents, num_agents, 2), dtype=np.int8)\n",
        "        self.comm_mask_buf = np.zeros((burn_in_steps+capacity+1, num_agents, num_agents), dtype=np.bool)\n",
        "        self.q_buf = np.zeros((capacity+1, action_dim), dtype=np.float32)\n",
        "\n",
        "        self.capacity = capacity\n",
        "        self.size = 0\n",
        "\n",
        "        self.obs_buf[:burn_in_steps+1] = init_obs\n",
        "\n",
        "    def add(self, q_val, action: int, last_act, reward: float, next_obs, hidden, relative_pos, comm_mask):\n",
        "        assert self.size < self.capacity\n",
        "\n",
        "        self.act_buf[self.size] = action\n",
        "        self.rew_buf[self.size] = reward\n",
        "        self.obs_buf[self.burn_in_steps+self.size+1] = next_obs\n",
        "        self.last_act_buf[self.burn_in_steps+self.size+1] = last_act\n",
        "        self.q_buf[self.size] = q_val\n",
        "        self.hidden_buf[self.burn_in_steps+self.size+1] = hidden\n",
        "        self.relative_pos_buf[self.burn_in_steps+self.size] = relative_pos\n",
        "        self.comm_mask_buf[self.burn_in_steps+self.size] = comm_mask\n",
        "\n",
        "        self.size += 1\n",
        "\n",
        "    def finish(self, last_q_val=None, last_relative_pos=None, last_comm_mask=None):\n",
        "        forward_steps = min(self.size, self.forward_steps)\n",
        "        cumulated_gamma = [gamma**forward_steps for _ in range(self.size-forward_steps)]\n",
        "\n",
        "        # last q value is None if done\n",
        "        if last_q_val is None:\n",
        "            done = True\n",
        "            cumulated_gamma.extend([0 for _ in range(forward_steps)])\n",
        "\n",
        "        else:\n",
        "            done = False\n",
        "            self.q_buf[self.size] = last_q_val\n",
        "            self.relative_pos_buf[self.burn_in_steps+self.size] = last_relative_pos\n",
        "            self.comm_mask_buf[self.burn_in_steps+self.size] = last_comm_mask\n",
        "            cumulated_gamma.extend([gamma**i for i in reversed(range(1, forward_steps+1))])\n",
        "\n",
        "\n",
        "        num_chunks = math.ceil(self.size/chunk_capacity)\n",
        "\n",
        "        cumulated_gamma = np.array(cumulated_gamma, dtype=np.float16)\n",
        "        self.obs_buf = self.obs_buf[:self.burn_in_steps+self.size+1]\n",
        "        self.last_act_buf = self.last_act_buf[:self.burn_in_steps+self.size+1]\n",
        "        self.act_buf = self.act_buf[:self.size]\n",
        "        self.rew_buf = self.rew_buf[:self.size+self.forward_steps-1]\n",
        "        self.hidden_buf = self.hidden_buf[:self.size]\n",
        "        self.relative_pos_buf = self.relative_pos_buf[:self.burn_in_steps+self.size+1]\n",
        "        self.comm_mask_buf = self.comm_mask_buf[:self.burn_in_steps+self.size+1]\n",
        "\n",
        "        self.rew_buf = np.convolve(self.rew_buf,\n",
        "                                [gamma**(self.forward_steps-1-i) for i in range(self.forward_steps)],\n",
        "                                'valid').astype(np.float16)\n",
        "\n",
        "        # caculate td errors for prioritized experience replay\n",
        "\n",
        "        max_q = np.max(self.q_buf[forward_steps:self.size+1], axis=1)\n",
        "        max_q = np.concatenate((max_q, np.array([max_q[-1] for _ in range(forward_steps-1)])))\n",
        "\n",
        "        target_q = self.q_buf[np.arange(self.size), self.act_buf]\n",
        "        td_errors = np.zeros(num_chunks*self.chunk_capacity, dtype=np.float32)\n",
        "        td_errors[:self.size] = np.abs(self.rew_buf+max_q*cumulated_gamma-target_q).clip(1e-6)\n",
        "        sizes = np.array([min(self.chunk_capacity, self.size-i*self.chunk_capacity) for i in range(num_chunks)], dtype=np.uint8)\n",
        "\n",
        "        data = EpisodeData(self.actor_id, self.num_agents, self.map_len, self.obs_buf, self.last_act_buf, self.act_buf,\n",
        "                    self.rew_buf, self.hidden_buf, self.relative_pos_buf, self.comm_mask_buf, cumulated_gamma, td_errors, sizes, done)\n",
        "\n",
        "        return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXMfarnM6TY0"
      },
      "source": [
        "#model.py k means"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "oD0ftilU7Zbl"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUWLTm_S6WIp",
        "outputId": "00ae6b19-4e94-4497-942d-01fbf88a8132"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-6-1195569359.py:197: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  @autocast()\n",
            "/usr/local/lib/python3.11/dist-packages/torch/amp/autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from typing import Dict\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.cuda.amp import autocast\n",
        "\n",
        "class CommLayer(nn.Module):\n",
        "    def __init__(self, input_dim=hidden_dim, message_dim=32, pos_embed_dim=16, num_heads=4):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.message_dim = message_dim\n",
        "        self.pos_embed_dim = pos_embed_dim\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        self.norm = nn.LayerNorm(input_dim)\n",
        "\n",
        "        self.position_embeddings = nn.Linear((2*obs_radius+1)**2, pos_embed_dim)\n",
        "\n",
        "        self.message_key = nn.Linear(input_dim+pos_embed_dim, message_dim * num_heads)\n",
        "        self.message_value = nn.Linear(input_dim+pos_embed_dim, message_dim * num_heads)\n",
        "        self.hidden_query = nn.Linear(input_dim, message_dim * num_heads)\n",
        "\n",
        "        self.head_agg = nn.Linear(message_dim * num_heads, message_dim * num_heads)\n",
        "\n",
        "        self.update = nn.GRUCell(num_heads*message_dim, input_dim)\n",
        "\n",
        "    def position_embed(self, relative_pos, dtype, device):\n",
        "\n",
        "        batch_size, num_agents, _, _ = relative_pos.size()\n",
        "        # mask out out of FOV agent\n",
        "        relative_pos[(relative_pos.abs() > obs_radius).any(3)] = 0\n",
        "\n",
        "        one_hot_position = torch.zeros((batch_size*num_agents*num_agents, 9*9), dtype=dtype, device=device)\n",
        "        relative_pos += obs_radius\n",
        "        relative_pos = relative_pos.reshape(batch_size*num_agents*num_agents, 2)\n",
        "        relative_pos_idx = relative_pos[:, 0] + relative_pos[:, 1]*9\n",
        "\n",
        "        one_hot_position[torch.arange(batch_size*num_agents*num_agents), relative_pos_idx.long()] = 1\n",
        "        position_embedding = self.position_embeddings(one_hot_position)\n",
        "\n",
        "        return position_embedding\n",
        "\n",
        "    def forward(self, hidden, relative_pos, comm_mask):\n",
        "        batch_size, num_agents, hidden_dim = hidden.size()\n",
        "        attn_mask = (comm_mask==False).unsqueeze(3).unsqueeze(4)\n",
        "        relative_pos = relative_pos.clone()\n",
        "\n",
        "        position_embedding = self.position_embed(relative_pos, hidden.dtype, hidden.device)\n",
        "\n",
        "        input = hidden\n",
        "\n",
        "        hidden = self.norm(hidden)\n",
        "\n",
        "        hidden_q = self.hidden_query(hidden).view(batch_size, 1, num_agents, self.num_heads, self.message_dim) # batch_size x num_agents x message_dim*num_heads\n",
        "\n",
        "        message_input = hidden.repeat_interleave(num_agents, dim=1).view(batch_size*num_agents*num_agents, hidden_dim)\n",
        "        message_input = torch.cat((message_input, position_embedding), dim=1)\n",
        "        message_input = message_input.view(batch_size, num_agents, num_agents, self.input_dim+self.pos_embed_dim)\n",
        "        message_k = self.message_key(message_input).view(batch_size, num_agents, num_agents, self.num_heads, self.message_dim)\n",
        "        message_v = self.message_value(message_input).view(batch_size, num_agents, num_agents, self.num_heads, self.message_dim)\n",
        "\n",
        "        # attention\n",
        "        attn_score = (hidden_q * message_k).sum(4, keepdim=True) / self.message_dim**0.5 # batch_size x num_agents x num_agents x self.num_heads x 1\n",
        "        attn_score.masked_fill_(attn_mask, torch.finfo(attn_score.dtype).min)\n",
        "        attn_weights = F.softmax(attn_score, dim=1)\n",
        "\n",
        "        # agg\n",
        "        agg_message = (message_v * attn_weights).sum(1).view(batch_size, num_agents, self.num_heads*self.message_dim)\n",
        "        agg_message = self.head_agg(agg_message)\n",
        "\n",
        "        # update hidden with request message\n",
        "        input = input.view(-1, hidden_dim)\n",
        "        agg_message = agg_message.view(batch_size*num_agents, self.num_heads*self.message_dim)\n",
        "        updated_hidden = self.update(agg_message, input)\n",
        "\n",
        "        # some agents may not receive message, keep it as original\n",
        "        update_mask = comm_mask.any(1).view(-1, 1)\n",
        "        hidden = torch.where(update_mask, updated_hidden, input)\n",
        "        hidden = hidden.view(batch_size, num_agents, hidden_dim)\n",
        "\n",
        "        return hidden\n",
        "\n",
        "\n",
        "\n",
        "class CommBlock(nn.Module):\n",
        "    def __init__(self, hidden_dim=hidden_dim, message_dim=128, pos_embed_dim=16):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.message_dim = message_dim\n",
        "        self.pos_embed_dim = pos_embed_dim\n",
        "\n",
        "        self.request_comm = CommLayer()\n",
        "        self.reply_comm = CommLayer()\n",
        "\n",
        "\n",
        "    def forward(self, latent, relative_pos, comm_mask):\n",
        "        '''\n",
        "        latent shape: batch_size x num_agents x latent_dim\n",
        "        relative_pos shape: batch_size x num_agents x num_agents x 2\n",
        "        comm_mask shape: batch_size x num_agents x num_agents\n",
        "        '''\n",
        "\n",
        "        batch_size, num_agents, latent_dim = latent.size()\n",
        "\n",
        "        assert relative_pos.size() == (batch_size, num_agents, num_agents, 2), relative_pos.size()\n",
        "        assert comm_mask.size() == (batch_size, num_agents, num_agents), comm_mask.size()\n",
        "\n",
        "        if torch.sum(comm_mask).item() == 0:\n",
        "            return latent\n",
        "\n",
        "        hidden = self.request_comm(latent, relative_pos, comm_mask)\n",
        "\n",
        "        comm_mask = torch.transpose(comm_mask, 1, 2)\n",
        "\n",
        "        hidden = self.reply_comm(hidden, relative_pos, comm_mask)\n",
        "\n",
        "        return hidden\n",
        "\n",
        "class Network(nn.Module):\n",
        "    def __init__(self, input_shape=obs_shape, selective_comm=selective_comm):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.latent_dim = self.hidden_dim + 5\n",
        "        self.obs_shape = input_shape\n",
        "        self.selective_comm = selective_comm\n",
        "\n",
        "        self.obs_encoder = nn.Sequential(\n",
        "            nn.Conv2d(input_shape[0], 64, 3, 1),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            nn.Conv2d(64, 128, 3, 1),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            nn.Conv2d(128, 192, 3, 1),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            nn.Conv2d(192, 256, 3, 1),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            nn.Flatten(),\n",
        "        )\n",
        "\n",
        "        self.recurrent = nn.GRUCell(self.latent_dim, self.hidden_dim)\n",
        "        self.comm = CommBlock(self.hidden_dim)\n",
        "\n",
        "        self.hidden = None\n",
        "\n",
        "        # dueling q structure\n",
        "        self.adv = nn.Linear(self.hidden_dim, 5)\n",
        "        self.state = nn.Linear(self.hidden_dim, 1)\n",
        "\n",
        "    @torch.no_grad()\n",
        "\n",
        "    def step(self, obs, last_act, pos):\n",
        "      num_agents = obs.size(0)\n",
        "      agent_indexing = torch.arange(num_agents)\n",
        "      relative_pos = pos.unsqueeze(0) - pos.unsqueeze(1)\n",
        "\n",
        "      in_obs_mask = (relative_pos.abs() <= obs_radius).all(2)\n",
        "      in_obs_mask[agent_indexing, agent_indexing] = 0\n",
        "\n",
        "      group_size = 4\n",
        "      num_groups = max(1, num_agents // group_size)\n",
        "\n",
        "      pos_np = pos.cpu().numpy()\n",
        "      kmeans = KMeans(n_clusters=num_groups, n_init='auto').fit(pos_np)\n",
        "      labels = kmeans.labels_\n",
        "\n",
        "      comm_mask = torch.zeros((num_agents, num_agents), dtype=bool)\n",
        "      for i in range(num_agents):\n",
        "          for j in range(num_agents):\n",
        "              if i != j and labels[i] == labels[j]:\n",
        "                  comm_mask[i, j] = True\n",
        "\n",
        "      comm_mask = comm_mask & in_obs_mask\n",
        "\n",
        "      latent = self.obs_encoder(obs)\n",
        "      latent = torch.cat((latent, last_act), dim=1)\n",
        "\n",
        "      if self.hidden is None:\n",
        "          self.hidden = self.recurrent(latent)\n",
        "      else:\n",
        "          self.hidden = self.recurrent(latent, self.hidden)\n",
        "\n",
        "      self.hidden = self.comm(self.hidden.unsqueeze(0), relative_pos.unsqueeze(0), comm_mask.unsqueeze(0))\n",
        "      self.hidden = self.hidden.squeeze(0)\n",
        "\n",
        "      adv_val = self.adv(self.hidden)\n",
        "      state_val = self.state(self.hidden)\n",
        "      q_val = state_val + adv_val - adv_val.mean(1, keepdim=True)\n",
        "\n",
        "      actions = torch.argmax(q_val, 1).tolist()\n",
        "\n",
        "      return actions, q_val.numpy(), self.hidden.numpy(), relative_pos.numpy(), comm_mask.numpy()\n",
        "\n",
        "\n",
        "    def reset(self):\n",
        "        self.hidden = None\n",
        "\n",
        "    @autocast()\n",
        "    def forward(self, obs, last_act, steps, hidden, relative_pos, comm_mask):\n",
        "        '''\n",
        "        used for training\n",
        "        '''\n",
        "        # obs shape: seq_len, batch_size, num_agents, obs_shape\n",
        "        # relative_pos shape: batch_size, seq_len, num_agents, num_agents, 2\n",
        "        seq_len, batch_size, num_agents, *_ = obs.size()\n",
        "\n",
        "        obs = obs.view(seq_len*batch_size*num_agents, *self.obs_shape)\n",
        "        last_act = last_act.view(seq_len*batch_size*num_agents, action_dim)\n",
        "\n",
        "        latent = self.obs_encoder(obs)\n",
        "        latent = torch.cat((latent, last_act), dim=1)\n",
        "        latent = latent.view(seq_len, batch_size*num_agents, self.latent_dim)\n",
        "\n",
        "        hidden_buffer = []\n",
        "        for i in range(seq_len):\n",
        "            # hidden size: batch_size*num_agents x self.hidden_dim\n",
        "            hidden = self.recurrent(latent[i], hidden)\n",
        "            hidden = hidden.view(batch_size, num_agents, self.hidden_dim)\n",
        "            hidden = self.comm(hidden, relative_pos[:, i], comm_mask[:, i])\n",
        "            # only hidden from agent 0\n",
        "            hidden_buffer.append(hidden[:, 0])\n",
        "            hidden = hidden.view(batch_size*num_agents, self.hidden_dim)\n",
        "\n",
        "        # hidden buffer size: batch_size x seq_len x self.hidden_dim\n",
        "        hidden_buffer = torch.stack(hidden_buffer).transpose(0, 1)\n",
        "\n",
        "        # hidden size: batch_size x self.hidden_dim\n",
        "        hidden = hidden_buffer[torch.arange(batch_size), steps-1]\n",
        "\n",
        "        adv_val = self.adv(hidden)\n",
        "        state_val = self.state(hidden)\n",
        "\n",
        "        q_val = state_val + adv_val - adv_val.mean(1, keepdim=True)\n",
        "\n",
        "        return q_val"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MO3XxtIx_TVd"
      },
      "source": [
        "#test.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aw42gony_U5m",
        "outputId": "ba85bf86-0cbb-4383-dae4-36fd07119392"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "128000\n",
            "----------test model 128000----------\n",
            "test set: 40 length 4 agents 0.3 density\n",
            "success rate: 99.50%\n",
            "average step: 49.54\n",
            "communication times: 35.5\n",
            "\n",
            "test set: 40 length 8 agents 0.3 density\n",
            "success rate: 100.00%\n",
            "average step: 58.35\n",
            "communication times: 203.55\n",
            "\n",
            "test set: 40 length 16 agents 0.3 density\n",
            "success rate: 99.00%\n",
            "average step: 72.4\n",
            "communication times: 1017.06\n",
            "\n",
            "test set: 40 length 32 agents 0.3 density\n",
            "success rate: 80.00%\n",
            "average step: 133.235\n",
            "communication times: 6575.38\n",
            "\n",
            "test set: 40 length 64 agents 0.3 density\n",
            "success rate: 19.50%\n",
            "average step: 242.655\n",
            "communication times: 38113.65\n",
            "\n",
            "test set: 80 length 4 agents 0.3 density\n",
            "success rate: 100.00%\n",
            "average step: 92.895\n",
            "communication times: 18.92\n",
            "\n",
            "test set: 80 length 8 agents 0.3 density\n",
            "success rate: 99.00%\n",
            "average step: 108.83\n",
            "communication times: 111.66\n",
            "\n",
            "test set: 80 length 16 agents 0.3 density\n",
            "success rate: 97.50%\n",
            "average step: 119.81\n",
            "communication times: 463.6\n",
            "\n",
            "test set: 80 length 32 agents 0.3 density\n",
            "success rate: 92.00%\n",
            "average step: 143.115\n",
            "communication times: 2324.01\n",
            "\n",
            "test set: 80 length 64 agents 0.3 density\n",
            "success rate: 54.00%\n",
            "average step: 212.4\n",
            "communication times: 12228.56\n",
            "\n"
          ]
        }
      ],
      "source": [
        "'''create test set and test model'''\n",
        "import os\n",
        "import random\n",
        "import pickle\n",
        "from typing import Tuple, Union\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\", UserWarning)\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.multiprocessing as mp\n",
        "\n",
        "torch.manual_seed(test_seed)\n",
        "np.random.seed(test_seed)\n",
        "random.seed(test_seed)\n",
        "DEVICE = torch.device('cpu')\n",
        "torch.set_num_threads(1)\n",
        "\n",
        "def create_test(test_env_settings: Tuple = test_env_settings, num_test_cases: int = num_test_cases):\n",
        "    '''\n",
        "    create test set\n",
        "    '''\n",
        "\n",
        "    for map_length, num_agents, density in test_env_settings:\n",
        "\n",
        "        name = f'./test_set/{map_length}length_{num_agents}agents_{density}density.pth'\n",
        "        print(f'-----{map_length}length {num_agents}agents {density}density-----')\n",
        "\n",
        "        tests = []\n",
        "\n",
        "        env = Environment(fix_density=density, num_agents=num_agents, map_length=map_length)\n",
        "\n",
        "        for _ in tqdm(range(num_test_cases)):\n",
        "            tests.append((np.copy(env.map), np.copy(env.agents_pos), np.copy(env.goals_pos)))\n",
        "            env.reset(num_agents=num_agents, map_length=map_length)\n",
        "        print()\n",
        "\n",
        "        with open(name, 'wb') as f:\n",
        "            pickle.dump(tests, f)\n",
        "\n",
        "\n",
        "\n",
        "def test_model(model_range: Union[int, tuple], test_set: Tuple = test_env_settings):\n",
        "    '''\n",
        "    test model in 'saved_models' folder\n",
        "    '''\n",
        "    network = Network()\n",
        "    network.eval()\n",
        "    network.to(DEVICE)\n",
        "    pool = mp.Pool(mp.cpu_count()//2)\n",
        "    print(model_range)\n",
        "\n",
        "    if isinstance(model_range, int):\n",
        "        state_dict = torch.load(os.path.join(save_path, f'{model_range}.pth'), map_location=DEVICE)\n",
        "        network.load_state_dict(state_dict)\n",
        "        network.eval()\n",
        "        network.share_memory()\n",
        "\n",
        "\n",
        "        print(f'----------test model {model_range}----------')\n",
        "\n",
        "        for case in test_set:\n",
        "            print(f\"test set: {case[0]} length {case[1]} agents {case[2]} density\")\n",
        "            with open('./test_set/{}length_{}agents_{}density.pth'.format(case[0], case[1], case[2]), 'rb') as f:\n",
        "                tests = pickle.load(f)\n",
        "\n",
        "            tests = [(test, network) for test in tests]\n",
        "            ret = pool.map(test_one_case, tests)\n",
        "\n",
        "            success, steps, num_comm = zip(*ret)\n",
        "\n",
        "\n",
        "            print(\"success rate: {:.2f}%\".format(sum(success)/len(success)*100))\n",
        "            print(\"average step: {}\".format(sum(steps)/len(steps)))\n",
        "            print(\"communication times: {}\".format(sum(num_comm)/len(num_comm)))\n",
        "            print()\n",
        "\n",
        "    elif isinstance(model_range, tuple):\n",
        "\n",
        "        for model_name in range(model_range[0], model_range[1]+1, save_interval):\n",
        "            state_dict = torch.load(os.path.join(save_path, f'{model_name}.pth'), map_location=DEVICE)\n",
        "            network.load_state_dict(state_dict)\n",
        "            network.eval()\n",
        "            network.share_memory()\n",
        "\n",
        "\n",
        "            print(f'----------test model {model_name}----------')\n",
        "\n",
        "            for case in test_set:\n",
        "                print(f\"test set: {case[0]} length {case[1]} agents {case[2]} density\")\n",
        "                with open(f'./test_set/{case[0]}length_{case[1]}agents_{case[2]}density.pth', 'rb') as f:\n",
        "                    tests = pickle.load(f)\n",
        "\n",
        "                tests = [(test, network) for test in tests]\n",
        "                ret = pool.map(test_one_case, tests)\n",
        "\n",
        "\n",
        "                success, steps, num_comm = zip(*ret)\n",
        "\n",
        "                print(\"success rate: {:.2f}%\".format(sum(success)/len(success)*100))\n",
        "                print(\"average step: {}\".format(sum(steps)/len(steps)))\n",
        "                print(\"communication times: {}\".format(sum(num_comm)/len(num_comm)))\n",
        "                print()\n",
        "\n",
        "            print('\\n')\n",
        "\n",
        "def test_one_case(args):\n",
        "\n",
        "    env_set, network = args\n",
        "\n",
        "    env = Environment()\n",
        "    env.load(env_set[0], env_set[1], env_set[2])\n",
        "    obs, last_act, pos = env.observe()\n",
        "\n",
        "    done = False\n",
        "    network.reset()\n",
        "\n",
        "    step = 0\n",
        "    num_comm = 0\n",
        "    while not done and env.steps < max_episode_length:\n",
        "        actions, _, _, _, comm_mask = network.step(torch.as_tensor(obs.astype(np.float32)).to(DEVICE),\n",
        "                                                    torch.as_tensor(last_act.astype(np.float32)).to(DEVICE),\n",
        "                                                    torch.as_tensor(pos.astype(int)))\n",
        "        (obs, last_act, pos), _, done, _ = env.step(actions)\n",
        "        step += 1\n",
        "        num_comm += np.sum(comm_mask)\n",
        "\n",
        "    return np.array_equal(env.agents_pos, env.goals_pos), step, num_comm\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def code_test():\n",
        "    env = Environment()\n",
        "    network = Network()\n",
        "    network.eval()\n",
        "    obs, last_act, pos = env.observe()\n",
        "    network.step(torch.as_tensor(obs.astype(np.float32)).to(DEVICE),\n",
        "                                                    torch.as_tensor(last_act.astype(np.float32)).to(DEVICE),\n",
        "                                                    torch.as_tensor(pos.astype(int)))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    # load trained model and reproduce results in paper\n",
        "    test_model(128000)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}